{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment nº2\n",
    "### Implementation and Analysis of Decision Tree Induction for Supervised Machine Learning\n",
    "#### Work assembled by Alejandro Gonçalves, Francisca Mihalache and Pedro Fernandes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of contents <a name=\"contents\"></a>\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Datasets](#datasets)\n",
    "   - 2.1. [Restaurant](#restaurant1)\n",
    "   - 2.2. [Weather](#weather1)\n",
    "   - 2.3. [Iris](#iris1)\n",
    "   - 2.4. [Connect4](#connect41)\n",
    "3. [ID3](#id3)\n",
    "   - 3.1. [Entropy](#entropy)\n",
    "   - 3.2. [Information Gain](#information_gain)\n",
    "   - 3.3. [Example](#eg1)\n",
    "4. [ID3 Implementation](#id3-implementation)\n",
    "    - 4.1. [Restaurant](#restaurant2)\n",
    "    - 4.2. [Weather](#weather2)\n",
    "    - 4.3. [Iris](#iris2)\n",
    "    - 4.4. [Connect4](#connect42)\n",
    "5. [New Inputs](#inputs)\n",
    "6. [Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a name=\"introduction\"></a>\n",
    "The goal is to implement the ID3 algorithm to learn decision trees from data, ensuring the program can both build the tree and classify new instances across different datasets, without using automatic libraries.\n",
    "\n",
    "The focus is on learning the theoretical concepts and practically applying them to solve classification tasks effectively.\n",
    "\n",
    "*The code can be ran sequentially. When pressing *run all*, if it gets stuck somewhere, re-running the cell individually will probably fix it.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import math\n",
    "from collections import Counter\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets <a name=\"datasets\"></a>\n",
    "[[go back to the top]](#contents)\n",
    "\n",
    "In this section, we will dive into the details of our datasets, exploring the nature of the data and the predictive outcomes they are associated with.\n",
    "\n",
    "**Note**: for those with ID's, that column is dropped, since it has no predictive value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurant <a name=\"restaurant1\"></a>\n",
    "[[go back to the topic]](#datasets)\n",
    "\n",
    "This csv contains some examples of decision making. Given some information, the target is to decide whether the client waits or not.\n",
    "Each line, each instance contains 12 attributes, split among these categories.\n",
    "\n",
    "1. **Identification and Basic Information**:\n",
    "   - ID: Personal identifier for each restaurant entry.\n",
    "   - Type: Classification of the type of cuisine offered by the restaurant.\n",
    "\n",
    "2. **Dining Options within the Restaurant**:\n",
    "   - Alt: Availability of Alt dining options.\n",
    "   - Bar: Presence of a bar within the restaurant premises.\n",
    "\n",
    "3. **Temporal and Climate Conditions**:\n",
    "   - Fri: Indication of whether it is Friday or not.\n",
    "   - Rain: Weather condition, specifically rainy weather.\n",
    "\n",
    "4. **Customer Attributes**:\n",
    "   - Hun: Customer's hunger level.\n",
    "   - Pat: Patron density, indicating how crowded the restaurant is.\n",
    "\n",
    "5. **Service and Pricing**:\n",
    "   - Price: Price range of the restaurant.\n",
    "   - Res: Reservation availability for customers.\n",
    "\n",
    "6. **Wait Time**:\n",
    "   - Est: Estimated waiting time for a table.\n",
    "  \n",
    "7. **Decision Making**:\n",
    "   - Class: Target variable indicating whether a customer decides to wait for a table ('Yes') or not ('No').\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Checking and cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('restaurant.csv')\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "Checking if there's any missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df1.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping Booleans to Integers\n",
    "\n",
    "By replacing String values with small integers, it's easier for the program to process boolean values, and even the ones that have more than 2 options, for example the attribute **Price**.\n",
    "\n",
    "After the last evaluation, we need to handle the empty (None) values in the Pat attribute. (None != 'None').\n",
    "\n",
    "Again, if there is need to do so, just uncommenting what's below is enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trivial python implementation to solve the labels with empty values\n",
    "def map_pat_to_int(value):\n",
    "    mapping = {'None': 'None', 'Some': 'Some', 'Full': 'Full'}\n",
    "    return mapping.get(value, -1)  # return -1 for empty values\n",
    "\n",
    "df1['Pat'] = df1['Pat'].apply(map_pat_to_int)\n",
    "df1 = df1.drop('ID', axis=1)\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather <a name=\"weather1\"></a>\n",
    "[[go back to the topic]](#datasets)\n",
    "\n",
    "The 'weather' dataset holds a range of meteorological conditions and their impact on the decision to engage in an outdoor activity (specifically playing a sport).\n",
    "\n",
    "1. **ID** - Unique instance identification\n",
    "  \n",
    "2. **Weather Conditions**\n",
    "   - Weather: sunny, overcast, rainy\n",
    "   - Temperature: temperature (F) value\n",
    "   - Humidity: percentage, ranging from 0-100\n",
    "   - Windy: presence or absence \n",
    "\n",
    "3. **Decision** - Play or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Weather</th>\n",
       "      <th>Temp</th>\n",
       "      <th>Humidity</th>\n",
       "      <th>Windy</th>\n",
       "      <th>Play</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sunny</td>\n",
       "      <td>85</td>\n",
       "      <td>85</td>\n",
       "      <td>False</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sunny</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>True</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>overcast</td>\n",
       "      <td>83</td>\n",
       "      <td>86</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>rainy</td>\n",
       "      <td>70</td>\n",
       "      <td>96</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>rainy</td>\n",
       "      <td>68</td>\n",
       "      <td>80</td>\n",
       "      <td>False</td>\n",
       "      <td>yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Weather  Temp  Humidity  Windy Play\n",
       "0     sunny    85        85  False   no\n",
       "1     sunny    80        90   True   no\n",
       "2  overcast    83        86  False  yes\n",
       "3     rainy    70        96  False  yes\n",
       "4     rainy    68        80  False  yes"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.read_csv('weather.csv')\n",
    "df2 = df2.drop('ID', axis=1)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values\n",
    "Checking if there's any missing values, if so, those need to be handled accordingly. (There are none)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df2.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris <a name=\"iris1\"></a>\n",
    "[[go back to the topic]](#datasets)\n",
    "\n",
    "The 'iris' dataset, which is renowned for its introdutory role in the ML world, records the morphological measurements of *Iris* flowers. The attributes recorded are the following: \n",
    "\n",
    " 1. **ID** - Unique instance identification\n",
    "  \n",
    "2. **Features**\n",
    "   - Sepal Length\n",
    "   - Petal Length\n",
    "   - Sepal Width\n",
    "   - Petal Width\n",
    "\n",
    "3. **Decision** - Decide which species (Iris-setosa, Iris-versicolor, or Iris-virginica) the instance belongs to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('iris.csv')\n",
    "df3 = df3.drop('ID', axis=1)\n",
    "df3.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "Checking if there's any missing values, we conclude that there are none."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a **scatterplot matrix**\n",
    "Although not used, it's useful to check this datasets data properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.pairplot(df3, hue='class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note for the Iris dataset**: after careful search andevaluation, we concluded that the dataset is already clean, without outliers. For that reason, no further issues considering the csv will take place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect4 <a name=\"connect41\"></a>\n",
    "[[go back to the topic]](#datasets)\n",
    "\n",
    "The 'connect4' csv consists of multiple instances representing endgame states of the Connect Four game. The objective is to use these configurations to predict the outcome of the game - whether it ends in a win for the first player, a win for the second player, or a draw. \n",
    "\n",
    "This dataset is different fundamentaly, it does not contain attributes, but only the final state and the winner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = pd.read_csv('connect4.csv',header=0) \n",
    "\n",
    "df4.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing Values\n",
    "No missing values, so no need to access that problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df4.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3 <a name=\"id3\"></a>\n",
    "[[go back to the top]](#contents)\n",
    "\n",
    "A instance of the class ID3 will be defined only by the dataframe. Then, since all datasets are built the same, and all functions and the class logic keep on using the target (last column) and the attributes (excluding 1st and last), we define those in the *__init__*.\n",
    "\n",
    "All methods will have a commentary below briefly describing each method. When placing the cursor on a function call, there is a small description that helps the orientation through the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3:\n",
    "    def __init__(self, df):\n",
    "        '''Initializes the class, defines the target and attributes subset.'''\n",
    "        self.df = df\n",
    "        # self.tree = self._build_tree(df) # Pedro -> didn't work because of the order of definitions in the notebook I think, the idea was to, when defining the ID3 instance, creating the tree immediatly\n",
    "        self.atts = df.columns.tolist()[0:-1] \n",
    "        self.target = df.columns.tolist()[-1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ID3 decision tree algorithm strategically employs Information Gain to identify optimal points for splitting the data during the tree-building process. This selection is crucial as it influences the algorithm's ability to accurately classify data.\n",
    "\n",
    " To quantify IG, he uses the concept of **Entropy**. By calculating Entropy, we can evaluate the effectiveness of each potential split in **reducing uncertainty**, guiding the decision tree to more effective and informed splits that **enhance classification performance**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Entropy Calculation:\n",
    "- **Definition of Entropy:** Measures the amount of uncertainty in the dataset.\n",
    "- **Entropy Calculation Formula:**\n",
    "\n",
    "  $$ \n",
    "  \\text{Entropy}(S) = -\\sum_{i=1}^n p_i \\log_2(p_i) \n",
    "  $$\n",
    "  - **n:** Total number of classes in the target column.\n",
    "  - **p_i:** Probability of class i, or the ratio of \"number of rows with class i in the target column\" to the \"total number of rows\" in the dataset.\n",
    "  - **log_2(p_i):** Logarithm base 2 of p_i, used to calculate the entropy contribution of each class.\n",
    "\n",
    "\n",
    "#### Information Gain Assessment:\n",
    "- **Information Gain Calculation:** Determine the gain in information by computing the change in entropy before and after partitioning the dataset based on an attribute.\n",
    "\n",
    "  - **Information Gain Formula:**\n",
    "    $$\n",
    "    IG(S, A) = \\text{Entropy}(S)- \\sum \\left( \\frac{|S_v|}{|S|} \\cdot \\text{Entropy}(S_v) \\right)\n",
    "  $$\n",
    "    \n",
    "  In this formula:\n",
    "  - **S**  is the entire dataset.\n",
    "  - **A** is the attribute used for partitioning.\n",
    "  - **S_v**  is the size of the subset of S where the attribute A has value v.\n",
    "  - **|S|**  is the total size of the dataset.\n",
    "  - **Entropy(S)** is the entropy of the entire dataset S.\n",
    "  - **Entropy(S_v)** is the entropy of subset S_v. \n",
    "\n",
    "##### Attribute Selection:\n",
    "- **Optimal Attribute Choice:** Select the attribute with the maximal information gain for splitting. This is done by calculating the information gain for each attribute and comparing these values to identify the attribute that most effectively reduces uncertainty.\n",
    "\n",
    "##### Iterative Process:\n",
    "- **Repetition Until Completion:** Continuing the process iteratively, applying the steps of entropy calculation, information gain assessment, and attribute selection recursively on each subset of the dataset created by a decision node.\n",
    "\n",
    "- The process ends when all data at a node have the same class, no remaining attributes exist for further partitioning, or another stopping criterion is met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Entropy <a name=\"entropy\"></a>\n",
    "[[go back to the topic]](#id3)\n",
    "\n",
    "In order to do so, here's an implementation in python which calculates the entropy given the different values parsed as a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3(ID3):\n",
    "    def _entropy(self, class_counts):\n",
    "        \"\"\"Return the entropy given a list with the class counts.\"\"\"\n",
    "        total = sum(class_counts) \n",
    "        if total == 0:\n",
    "            return 0 \n",
    "\n",
    "        entropy = 0\n",
    "        for count in class_counts:\n",
    "            if count > 0:\n",
    "                probability = count / total\n",
    "                entropy -= probability * math.log2(probability)\n",
    "        #print(class_counts)\n",
    "        return entropy\n",
    "\n",
    "\n",
    "class_counts = [25, 25]\n",
    "a = ID3(df1)  \n",
    "entropy = a._entropy(class_counts)\n",
    "print(f\"Entropy of the distribution is: {entropy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to parse all the class counts as a list, and compare the different entropies, to choose the one that properly divides the dataset the most.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Information Gain <a name=\"information_gain\"></a>\n",
    "[[go back to the topic]](#id3)\n",
    "\n",
    "Now, using the previous function, we need to calculate the information gain for splitting the tree using the attribute_name, and the target_name.\n",
    "\n",
    "Next, there is an implementation in *python* which calculates the IG given the previously stated arguments.\n",
    "\n",
    "The function starts by calculating the weighted entropy, doing the summatory described above, and then doing the formula below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3(ID3):\n",
    "    def _calculate_entropy_of_split(self, df, target_attribute):\n",
    "        '''Auxiliar to _information_gain, parses the target att counts to entropy'''\n",
    "        counts = [len(df[df[target_attribute] == value]) for value in df[target_attribute].unique()]\n",
    "        return self._entropy(counts)\n",
    "\n",
    "    def _information_gain(self, df_subset, attribute_name):\n",
    "        '''Returns IG given the current subset and the attribute'''\n",
    "        total_entropy = self._calculate_entropy_of_split(df_subset, self.target)\n",
    "        weighted_entropy = 0\n",
    "\n",
    "        for value in df_subset[attribute_name].unique(): # for each unique value in the df_subset attribute\n",
    "            subset = df_subset[df_subset[attribute_name] == value] # \n",
    "            subset_entropy = self._calculate_entropy_of_split(subset, self.target)\n",
    "            weighted_entropy += (len(subset) / len(df_subset)) * subset_entropy\n",
    "\n",
    "        return total_entropy - weighted_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plurality Values\n",
    "\n",
    "According to page 660 of the manual, **if** there is need to break ties, we pick the **most common output value** among a set of examples.\n",
    "\n",
    "Panda module has a built-in function to order by the most common, so we just need to adapt it. Although not now, it might be used for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3(ID3):\n",
    "    def _plurality_value(self, target_attribute):\n",
    "        \"\"\"Return the most common value in the target attribute.\"\"\"\n",
    "        return Counter(self.df[target_attribute]).most_common(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example <a name=\"eg1\"></a>\n",
    "[[go back to the topic]](#id3)\n",
    "\n",
    "Let's test the new function for the Restaurant csv, splitting on the **Alt** attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_name = 'Alt'\n",
    "\n",
    "unique_values = df1[column_name].unique()\n",
    "\n",
    "class_counts = []\n",
    "for arr in unique_values:\n",
    "    x = df1[df1[column_name] == arr].shape[0]\n",
    "    class_counts.append(x)\n",
    "\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are good to go.\n",
    "The entropy should be 1, as it splits the dataset evenly, but we will test the function now.\n",
    "\n",
    "Then, we can test the IG calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = ID3(df1)\n",
    "entr = b._entropy(class_counts)\n",
    "print(f\"Entropy for {class_counts} : {entr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ID3(df1)\n",
    "\n",
    "information_gains = {}\n",
    "\n",
    "for column in c.atts:\n",
    "    gain = c._information_gain(c.df, column)\n",
    "    information_gains[column] = gain\n",
    "\n",
    "# Choose the attribute with the highest information gain\n",
    "max_gain_attr = max(information_gains, key=information_gains.get)\n",
    "\n",
    "print(f\"\\nAttribute with the maximum information gain: {max_gain_attr} (IG: {information_gains[max_gain_attr]:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IG associated with each attribute is represented in the table below, in which we can assess that in fact, Pat has the highest IG score.\n",
    "\n",
    "\n",
    "\n",
    "| **Attribute**  | Alt | Bar | Fri | Hun | Pat | Price | Rain | Res | Type | Est |\n",
    "|------------|-----|-----|-----|-----|-----|-------|------|-----|------|-----|\n",
    "| **Information Gain**  | 0.000 | 0.000 | 0.021 | 0.196 | 0.541 | 0.196 | 0.000 | 0.021 | 0.000 | 0.208 |\n",
    "\n",
    "\n",
    "\n",
    "Now we have the base working for the ID3 algorithm, and can calculate the IG of the attributes correctly in each instance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choose the best attribute\n",
    "\n",
    "In order to do so, we just need to use the previously defined methods, to pick the index of the attribute with the highest IG. \n",
    "\n",
    "The ID3 method *choose_best_attribute* creates a dictionary that links all the (remaining) attributes to their respective IG, and returns the key (attribute name) of the one with the max IG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3(ID3):\n",
    "    def choose_best_attribute(self, df_subset):    \n",
    "        \"\"\"Returns, given the subset, the name of the attribute with the highest IG.\"\"\"    \n",
    "        remaining_attributes = [col for col in df_subset.columns if col != self.target] # debug/double-checking step\n",
    "\n",
    "        information_gains = {} # dict att -> IG(att)\n",
    "        for attr in remaining_attributes:\n",
    "            information_gains[attr] = self._information_gain(df_subset, attr)\n",
    "\n",
    "        return max(information_gains, key=information_gains.get)\n",
    "\n",
    "d = ID3(df1)\n",
    "subset = d.df  # simulating the first iteration, therefore we use the entire dataset\n",
    "best_attribute = d.choose_best_attribute(subset)\n",
    "\n",
    "print(\"Best Attribute for Splitting:\", best_attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3 Implementation <a name=\"id3-implementation\"></a> \n",
    "[[go back to the top]](#contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restaurant Implementation <a name=\"restaurant2\"></a> \n",
    "\n",
    "This is the simplest one. No adaptations are required, since the **values are discrete**, and not continuous values, and the **target class is binary**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3(ID3):\n",
    "    def _build_tree(self, data):\n",
    "        \"\"\"Constructs the nested dictionary for a specific dataframe, has the argument data for the recursion matter.\"\"\"\n",
    "        # debug statements from chatGPT\n",
    "\n",
    "        labels = data[data.columns[-1]].tolist()\n",
    "        class_counts = Counter(labels)\n",
    "\n",
    "        # Base cases\n",
    "        if len(set(labels)) == 1:\n",
    "            return [labels[0], class_counts[labels[0]]]  # returned 2 thing to help the debug below\n",
    "        if len(data.columns) == 1:\n",
    "            return [class_counts.most_common(1)[0][0], len(labels)] \n",
    "\n",
    "        # Choose the best attribute to split on\n",
    "        best_attribute = self.choose_best_attribute(data)\n",
    "        node = {best_attribute: {}}\n",
    "\n",
    "        complete_values = self.df[best_attribute].unique() # since French was missing, we need another \"flag\"\n",
    "        subset_values = data[best_attribute].unique()\n",
    "        # print(f\"Building tree on {best_attribute}, values: {complete_values}\")\n",
    "        \n",
    "        # missing_values = set(complete_values) - set(subset_values)\n",
    "        # print(missing_values)\n",
    "\n",
    "        for value in complete_values:\n",
    "            subset = data[data[best_attribute] == value]\n",
    "            if subset.empty:\n",
    "                # Assign the majority class if this value is missing in the subset\n",
    "                majority_class = class_counts.most_common(1)[0][0]\n",
    "                # print(f\"Value '{value}' missing. Assigning majority class '{majority_class}'\")\n",
    "                node[best_attribute][value] = [majority_class, 0]\n",
    "            else:\n",
    "                # Recursively build the tree for non-empty subsets\n",
    "                remaining_data = subset.drop(columns=[best_attribute])\n",
    "                node[best_attribute][value] = self._build_tree(remaining_data)\n",
    "\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the function usage \n",
    "e = ID3(df1)\n",
    "decision_tree_structure = e._build_tree(e.df)\n",
    "\n",
    "print(decision_tree_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can however define a recursive function with the corresponding indents, and make it look more like a tree. Since the formattation is not relevant to the project's objective, we used chatGPT to define the *_recursive_print_tree* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3(ID3):\n",
    "    def print_tree(self, tree_structure):\n",
    "        \"\"\"Public method to print the tree. Needed to solve the issue with the arguments and recursions.\"\"\"\n",
    "        print(\"{\")\n",
    "        self._recursive_print_tree(tree_structure, \"      \")\n",
    "        print(\"}\")\n",
    "\n",
    "    def _recursive_print_tree(self, tree, prefix=\"\", indent=\"      \"):\n",
    "        \"\"\"Private method that prints the nested dictionary as a tree, recursively. The indent is increasable for aesthetic reasons.\"\"\"\n",
    "        if isinstance(tree, dict):\n",
    "            for attribute, branches in tree.items():\n",
    "                print(prefix + f\"'{attribute}': {{\")\n",
    "                last_value = len(branches) - 1\n",
    "\n",
    "                for index, (value, subtree) in enumerate(branches.items()):\n",
    "                    is_last = index == last_value\n",
    "                    value_prefix = prefix + indent\n",
    "\n",
    "                    if isinstance(subtree, dict):\n",
    "                        print(value_prefix + f\"'{value}': {{\")\n",
    "                        self._recursive_print_tree(subtree, value_prefix + indent, indent)\n",
    "                        print(value_prefix + \"}\" + (\",\" if not is_last else \"\"))\n",
    "                    else:\n",
    "                        # Leaf node handling\n",
    "                        leaf_value = subtree[0] if isinstance(subtree, list) else subtree\n",
    "                        print(value_prefix + f\"'{value}': '{leaf_value}'\" + (\",\" if not is_last else \"\"))\n",
    "\n",
    "                print(prefix + \"}\" + (\",\" if not is_last else \"\"))\n",
    "\n",
    "        else:\n",
    "            # Handle single-value leaf nodes\n",
    "            leaf_value = tree[0] if isinstance(tree, list) else tree\n",
    "            print(prefix + f\"'{leaf_value}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final1 = ID3(df1)\n",
    "tree1 = final1._build_tree(final1.df)\n",
    "final1.print_tree(tree1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify examples for Restaurant\n",
    "\n",
    "We can create a new function, that iterates recursively the nested dictionary in order to classify a new example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3(ID3):\n",
    "    def classify(self, instance, node=None): # Classify a new instance using the decision tree.\n",
    "        '''Given an instance defined in a dictionary, iterates through the dictionary to classify new examples.'''\n",
    "        if not isinstance(node, dict):\n",
    "            return node  # Leaf node with classification label\n",
    "        attribute = next(iter(node))\n",
    "        value = instance.get(attribute)\n",
    "        next_branch = node.get(attribute).get(value, 'Unknown')\n",
    "        if isinstance(next_branch, dict):\n",
    "            return self.classify(instance, next_branch) # Iterate recursively to the next branch\n",
    "        else:\n",
    "            return next_branch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can define a new instance, and given the generated tree, will take the proper decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_instance = {\n",
    "    'Alt': 'Yes',\n",
    "    'Bar': 'No',\n",
    "    'Fri': 'Yes',\n",
    "    'Hun': 'No',\n",
    "    'Pat': 'Some',\n",
    "    'Price': '$$$',\n",
    "    'Rain': 'No',\n",
    "    'Res': 'Yes',\n",
    "    'Type': 'Italian',\n",
    "    'Est': '10-30'\n",
    "}\n",
    "\n",
    "f = ID3(df1)\n",
    "pred1 = f.classify(new_instance, decision_tree_structure)[0]\n",
    "print(\"Do them wait for a table?:\", pred1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify Bonus for Restaurant\n",
    "By reading the manual *(page 658, Figure 19.3)*, we realized **there is a complete tree for this decision**, and the tree we conducted is constructed following a short list of examples.\n",
    "\n",
    "We can evaluate the tree generated based on examples that follow the \"real/complete\" DT.\n",
    "\n",
    "The procedure is simply to **generate a finite number of instances** that follow that DT assignment, and give a score based on that, using the *classify()*, and comparing the expected result.\n",
    "\n",
    "#### 1) Complete Tree representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree_real = {\n",
    "    \"Pat\": {\n",
    "        \"None\": \"No\",\n",
    "        \"Some\": \"Yes\",\n",
    "        \"Full\": {\n",
    "            \"Est\": {\n",
    "                \"0-10\": \"Yes\",\n",
    "                \"10-30\": {\n",
    "                    \"Hun?\": {\n",
    "                        \"Yes\": {\n",
    "                            \"Alt\": {\n",
    "                                \"No\": \"Yes\",\n",
    "                                \"Yes\": {\n",
    "                                    \"Rain\": {\n",
    "                                        \"No\": \"No\",\n",
    "                                        \"Yes\": \"Yes\"\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        },\n",
    "                        \"No\": \"Yes\"\n",
    "                    }\n",
    "                },\n",
    "                \"30-60\": {\n",
    "                    \"Alt\": {\n",
    "                        \"Yes\": {\n",
    "                            \"Fri\": {\n",
    "                                \"Yes\": \"Yes\",\n",
    "                                \"No\": \"No\"\n",
    "                            }\n",
    "                        },\n",
    "                        \"No\": {\n",
    "                            \"Res\": {\n",
    "                                \"Yes\": \"Yes\",\n",
    "                                \"No\": {\n",
    "                                    \"Bar\": {\n",
    "                                        \"Yes\": \"Yes\",\n",
    "                                        \"No\": \"No\"\n",
    "                                    }\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \">60\": \"No\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "import random\n",
    "\n",
    "def create_new_rand_instance_rest(): \n",
    "    \"\"\"Returns a dictionary correspondent to a random assignment of attributes.\"\"\"\n",
    "    l1 = ['Yes', 'No']\n",
    "    l2 = ['None', 'Some', 'Full']\n",
    "    l3 = ['$', '$$', '$$$']\n",
    "    l4 = ['French', 'Thai', 'Italian', 'Burger']\n",
    "    l5 = ['0-10', '10-30', '30-60', '>60']\n",
    "\n",
    "    ret_dict = {\n",
    "        'Alt': random.choice(l1),\n",
    "        'Bar': random.choice(l1),\n",
    "        'Fri': random.choice(l1),\n",
    "        'Hun': random.choice(l1),\n",
    "        'Pat': random.choice(l2),\n",
    "        'Price': random.choice(l3),\n",
    "        'Rain': random.choice(l1),\n",
    "        'Res': random.choice(l1),\n",
    "        'Type': random.choice(l4),\n",
    "        'Est': random.choice(l5),\n",
    "    }\n",
    "\n",
    "    return ret_dict\n",
    "\n",
    "def normalize_response(response): # we were having issues, this is a trivial way to solve all formattation related problems\n",
    "    if 'Y' in str(response):\n",
    "        return 'Yes'\n",
    "    elif 'N' in str(response):\n",
    "        return 'No'\n",
    "    return response  \n",
    "\n",
    "def simulation_restaurant(total_simul):\n",
    "    inst_temp = ID3(df1)\n",
    "    csv_dt = inst_temp._build_tree(inst_temp.df)\n",
    "    real_dt = decision_tree_real\n",
    "\n",
    "    count_same = 0\n",
    "    for _ in range(total_simul):\n",
    "        x = create_new_rand_instance_rest()\n",
    "        cl1 = inst_temp.classify(x, csv_dt) # alusive to out generated tree\n",
    "        cl2 = inst_temp.classify(x, real_dt)\n",
    "\n",
    "        # print(normalize_response(cl1) + ' ' + normalize_response(cl2))\n",
    "        if normalize_response(cl1)==normalize_response(cl2):\n",
    "            count_same += 1\n",
    "    \n",
    "    return (count_same/total_simul)*100\n",
    "\n",
    "print(f\"The simulations calculate the generated tree to classify correctly {round(simulation_restaurant(10000))}% of the cases.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Implementation <a name=\"weather2\"></a> \n",
    "[[go back to the topic]](#id3-implementation)\n",
    "\n",
    "Now, instead of only having discrete values, they are now continuous, in the attributes **Temperature** and **Humidity**. In order to handle them, different rules must be written: instead of **choosing according to the value** we need to **find the best split value**, meaning the value in which the function splits the best.\n",
    "\n",
    "What this means is, instead of the only operator being \"=\" (eg: if 'Pat' = 'Some'), now there is <= and >, for example, if **Temperature<80**.\n",
    "\n",
    "*This notebook will be overwriting the ID3 class functions during the generalization process.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best split value\n",
    "\n",
    "There are several ways to choose the value to which we divide the class. For example, we can divide considering the **half of the interval range a-b**:\n",
    "\n",
    "$$\n",
    "\\text{α} = \\frac{{a + b}}{2}\n",
    "$$\n",
    "\n",
    "Another way is to use the value α such that the number of elements above and below it are equal.\n",
    "$$\n",
    "\\exists y \\text{ : } \\text{count}(X \\geq α) \\approx \\text{count}(X \\leq α)\n",
    "$$\n",
    "\n",
    "During this project we decided to **brute-force all possible values** already existing on that attribute, and choose the value α, that works on the highest information gain. To do so, we need to get all the values, divide into subsets and see which \"splitter\" works on the best IG.\n",
    "\n",
    "The approach was simple, in a copy, replace all the values above or below the split_value explicitly, and then parse those to the previous IG function. Although not optimal, the usage of *deepcopy* made easier for the project comprehension. \n",
    "\n",
    "To check if the replacement is produced properly, *uncomment all the code on the auxiliar function and run the next 2 code cells*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "class ID3(ID3):\n",
    "    def _calculate_best_split_value_aux(self, df_subset, att, split_value):\n",
    "        '''Auxiliar method, given a df_subset, a numerical attribute and the split value, it replaces the values (makes them discrete), and proceeds to calculate the IG as usual.'''\n",
    "        df_copy = copy.deepcopy(df_subset)\n",
    "\n",
    "        print_once_bool = False # lazy way to not print the head of the df a lot of times, prints only once since subset gets shorter\n",
    "        if df_subset.equals(self.df):\n",
    "            print_once_bool = True\n",
    "\n",
    "        for index in df_copy.index:\n",
    "            if df_copy.loc[index, att] <= split_value:\n",
    "                df_copy.loc[index, att] = 'leq'\n",
    "            else:\n",
    "                df_copy.loc[index, att] = 'more'\n",
    "        '''\n",
    "        if (print_once_bool): # print only once, in the first iteration\n",
    "            print(split_value)\n",
    "            print(self.df.head(10))\n",
    "            print(\"\\n\")\n",
    "            print(df_copy.head(10)) \n",
    "            print(\"\\n---------------------------------------------------------------\\n\")\n",
    "        '''\n",
    "        return self._information_gain(df_copy, att)\n",
    "        \n",
    "        \n",
    "    def _calculate_best_split_value(self, df_subset, att):\n",
    "        '''Given a subset, and a numerical attribute, returns a tuple with the best split value, and the respective IG when it splits the dataset there.'''\n",
    "        unique_values = sorted(df_subset[att].unique()) # list with the sorted unique values\n",
    "        #print(unique_values) \n",
    "        best_split_value = None\n",
    "        best_ig = float('-inf')\n",
    "\n",
    "        for i in range(1, len(unique_values)):\n",
    "            #split_value = (unique_values[i]) \n",
    "            split_value = (unique_values[i-1] + unique_values[i]) / 2 # if we want the midpoint\n",
    "            ig = self._calculate_best_split_value_aux(df_subset, att, split_value)\n",
    "\n",
    "            #print(f\"Current best value: {best_split_value} (IG: {best_ig}), testing for {split_value}... obtained IG:{ig}\")\n",
    "            if ig > best_ig:\n",
    "                best_ig = ig\n",
    "                best_split_value = split_value\n",
    "\n",
    "        return best_split_value, best_ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test the function, creating a instance for the weather.csv dataframe and testing on the **Temp** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ID3(df2)\n",
    "test._calculate_best_split_value(test.df, 'Temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adaptations for the code to run on the Weather dataframe\n",
    "\n",
    "Using this new implemented logic, some changes are required:\n",
    "- the *_choose_best_atribute* should be changed, and perform the conversion of the column to parse it to the IG function properly;\n",
    "- the *_build_tree* method needs to be able to check the type of data, and:\n",
    "   - if **categorical**, proceeds normally to get the IG;\n",
    "   - if **numerical**, needs to get the IG by testing all the different split values.;\n",
    "- the *_print_tree* associated method needs to print with the operators formatted as well, this method doesn't need to be changed, but the *_build_tree* needs to set the keys of the dictionary already with the operators.\n",
    "\n",
    "We can test the new adapted function along the way:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3(ID3):\n",
    "    def _choose_best_attribute(self, df_subset):        \n",
    "        \"\"\"Returns, given the subset, the name of the attribute with the highest IG.\"\"\"    \n",
    "        remaining_attributes = [col for col in df_subset.columns if col != self.target] # debug step\n",
    "\n",
    "        information_gains = {} # dict att -> IG(att)\n",
    "\n",
    "        for attr in remaining_attributes:\n",
    "            if df_subset[attr].dtype in ['int64', 'float64']:\n",
    "                #print(attr + ', value of the split: ' + str(self._calculate_best_split_value(df_subset, attr)[0]) + ' , IG of the split: ' + str(self._calculate_best_split_value(df_subset, attr)[1]))\n",
    "                information_gains[attr] = self._calculate_best_split_value(df_subset, attr)[1] # reminder that this method returns tuple(value, ig)\n",
    "            else:\n",
    "                information_gains[attr] =  self._information_gain(df_subset, attr)\n",
    "                #print(attr + ' with IG : ' + str(self._information_gain(df_subset, attr)))\n",
    "\n",
    "        return max(information_gains, key=information_gains.get)\n",
    "        # return best_attribute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create an instance for both **df1** and **df2**, and check if it's still all working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_inst = ID3(df1)\n",
    "df2_inst = ID3(df2)\n",
    "\n",
    "#print(df1_inst.choose_best_attribute(df1_inst.df))\n",
    "#print(df2_inst.target)\n",
    "print(f\"The attribute with the highest IG is: {df2_inst._choose_best_attribute(df2_inst.df)}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3(ID3):\n",
    "    def _build_tree(self, data):\n",
    "        \"\"\"Constructs the nested dictionary for a specific dataframe, has the argument data for the recursion matter.\"\"\"\n",
    "        labels = data[data.columns[-1]].tolist()\n",
    "        class_counts = Counter(labels)\n",
    "\n",
    "        # Base cases\n",
    "        if len(set(labels)) == 1:\n",
    "            return [labels[0], class_counts[labels[0]]]\n",
    "        if len(data.columns) == 1:\n",
    "            return [class_counts.most_common(1)[0][0], len(labels)]\n",
    "\n",
    "        # Choose the best attribute to split on\n",
    "        best_attribute = self._choose_best_attribute(data)\n",
    "        node = {best_attribute: {}}\n",
    "\n",
    "        # case 1 - continuous attributes separately\n",
    "        if data[best_attribute].dtype in ['int64', 'float64']:\n",
    "            best_split_value = self._calculate_best_split_value(data, best_attribute)[0] # since _calculate_best_split_value returns a tuple\n",
    "            subset1 = data[data[best_attribute] <= best_split_value]\n",
    "            subset2 = data[data[best_attribute] > best_split_value]\n",
    "            node[best_attribute]['<= ' + str(best_split_value)] = self._build_tree(subset1)\n",
    "            node[best_attribute]['> ' + str(best_split_value)] = self._build_tree(subset2)\n",
    "\n",
    "        # case 2 - discrete attributes, previous logic\n",
    "        else:\n",
    "            complete_values = self.df[best_attribute].unique()\n",
    "            for value in complete_values:\n",
    "                subset = data[data[best_attribute] == value]\n",
    "\n",
    "                if subset.empty:\n",
    "                    # Assign the majority class if the value is missing\n",
    "                    majority_class = class_counts.most_common(1)[0][0]\n",
    "                    node[best_attribute][value] = [majority_class, 0]\n",
    "                else:\n",
    "                    remaining_data = subset.drop(columns=[best_attribute])\n",
    "                    node[best_attribute][value] = self._build_tree(remaining_data)\n",
    "\n",
    "        return node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the updates, we can build the tree, using the same print_tree function as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste2 = ID3(df2)\n",
    "decision_tree_structure2 = teste2._build_tree(teste2.df)\n",
    "teste2.print_tree(decision_tree_structure2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To classify new examples, we need to adapt the *_classify* method to handle continuous values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3(ID3):\n",
    "    def _classify(self, instance, node=None):\n",
    "        '''Given an instance defined in a dictionary, iterates through the dictionary to classify new examples.'''\n",
    "        if not isinstance(node, dict):\n",
    "            return node[0]  # return the classification label\n",
    "\n",
    "        attribute = next(iter(node)) \n",
    "        if attribute in instance:  \n",
    "            attribute_value = instance[attribute] \n",
    "\n",
    "            # check for continuous or discrete attribute handling, and check operator within the key\n",
    "            if isinstance(attribute_value, (int, float)):  \n",
    "                for key in node[attribute]:\n",
    "                    if \"<=\" in key:\n",
    "                        split_value = float(key.split('<= ')[1])\n",
    "                        if attribute_value <= split_value:\n",
    "                            return self.classify(instance, node[attribute][key])\n",
    "                    elif \">\" in key:\n",
    "                        split_value = float(key.split('> ')[1])\n",
    "                        if attribute_value > split_value:\n",
    "                            return self.classify(instance, node[attribute][key])\n",
    "            else: # discrete, same logic as before\n",
    "                next_branch = node[attribute].get(attribute_value, 'Unknown')\n",
    "                if isinstance(next_branch, dict):\n",
    "                    return self.classify(instance, next_branch)  \n",
    "                else:\n",
    "                    # Correct handling of leaf nodes when not a dictionary\n",
    "                    return next_branch[0] if next_branch != 'Unknown' else 'Unknown'\n",
    "\n",
    "        return 'Unknown'  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Does he/she play with those conditions?: no\n"
     ]
    }
   ],
   "source": [
    "new_instance_2 = {\n",
    "    'Weather': 'sunny',\n",
    "    'Temp': 80,\n",
    "    'Humidity': 90,\n",
    "    'Windy': 'true'\n",
    "}\n",
    "\n",
    "y = ID3(df2)  # Ensure that y.tree is properly initialized with the decision tree\n",
    "classification_result = y._classify(new_instance_2, decision_tree_structure2)\n",
    "print(\"Does he/she play with those conditions?:\", classification_result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iris Implementation <a name=\"iris2\"></a> \n",
    "[[go back to the topic]](#id3-implementation)\n",
    "\n",
    "The key difference between this dataset and the previous 2 is that the **target class is non binary** (3 species). Although ID3 is essentially a binary classifier, we can extend and its logic to go around this problem.\n",
    "\n",
    "The simple application of what we had before, up until now results in the following tree:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teste3 = ID3(df3)\n",
    "decision_tree_structure3 = teste3._build_tree(teste3.df)\n",
    "teste3.print_tree(decision_tree_structure3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the tree is generated in some sense, it is wrong, since the code is made for binary targets.\n",
    "The methodology we'll perform first is to adapt the code for ONE *vs* ALL trees.\n",
    "\n",
    "Meaning we added a change to make it work and \"callable\":\n",
    "- Essencially, we will create 3 copies of the dataframe df3, each one for each possible species target;\n",
    "- then we will replace explicitly the target column values\n",
    "  - let's assume the target is *'Iris-Setosa'*\n",
    "  - the copy of the dataframe will replace the other 2 species with the same name, eg.: 'not-Iris-Setosa'\n",
    "  - the tree generated will then have a binary target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3_setosa = deepcopy(df3)\n",
    "df3_versicolor = deepcopy(df3)\n",
    "df3_virginica = deepcopy(df3)\n",
    "\n",
    "# make the target binary, in a 1v2 way\n",
    "df3_setosa['class'] = df3_setosa['class'].where(df3_setosa['class'] == 'Iris-setosa', 'Not-Iris-Setosa')\n",
    "df3_versicolor['class'] = df3_versicolor['class'].where(df3_versicolor['class'] == 'Iris-versicolor', 'Not-Iris-versicolor')\n",
    "df3_virginica['class'] = df3_virginica['class'].where(df3_virginica['class'] == 'Iris-virginica', 'Not-Iris-virginica')\n",
    "\n",
    "# create instances for each \n",
    "inst_setosa = ID3(df3_setosa)\n",
    "inst_versicolor = ID3(df3_versicolor)\n",
    "inst_virginica = ID3(df3_virginica)\n",
    "\n",
    "# create dt structures\n",
    "tree3_1 = inst_setosa._build_tree(inst_setosa.df)\n",
    "tree3_2 = inst_versicolor._build_tree(inst_versicolor.df)\n",
    "tree3_3 = inst_virginica._build_tree(inst_virginica.df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the trees\n",
    "print(\"Iris setosa tree: \")\n",
    "inst_setosa.print_tree(tree3_1)\n",
    "print(\"\\n------------------------------------------------------\\n\")\n",
    "print(\"Iris versicolor tree: \")\n",
    "inst_versicolor.print_tree(tree3_2)\n",
    "print(\"\\n------------------------------------------------------\\n\")\n",
    "print(\"Iris virginica tree: \")\n",
    "inst_virginica.print_tree(tree3_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the new instances, we can use the function, to test a new instance with the instances represented in the dictionary as previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_instance_3 = {\n",
    "    'sepallength': 4.8,\n",
    "    'sepalwidth': 3.1,\n",
    "    'petallength': 1.6,\n",
    "    'petalwidth': 0.2\n",
    "}\n",
    "\n",
    "z = ID3(df3)  # Ensure that y.tree is properly initialized with the decision tree\n",
    "classification_result_setosa = inst_setosa.classify(new_instance_3, tree3_1)\n",
    "res = 'Yes' if classification_result_setosa == 'Iris-setosa' else 'No'\n",
    "print(\"Is the instance a Iris-setosa?:\", res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to use all the datasets, and try to return an **answer that is coherent with all the trees**. Meaning the input can/will be accepted only when it is in accordance with the 3 trees.\n",
    "\n",
    "This will be achieved in the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 3 tree structures are stored in tree3_1, tree3_2, tree3_3\n",
    "\n",
    "'''\n",
    "# code above as a reminder\n",
    "inst_setosa = ID3(df3_setosa)\n",
    "inst_versicolor = ID3(df3_versicolor)\n",
    "inst_virginica = ID3(df3_virginica)\n",
    "'''\n",
    "\n",
    "new_instance_3 = {\n",
    "    'sepallength': 4.8,\n",
    "    'sepalwidth': 3.1,\n",
    "    'petallength': 1.6,\n",
    "    'petalwidth': 0.2\n",
    "}\n",
    "\n",
    "# booleans definition\n",
    "isSetosa = False\n",
    "isVersicolor = False\n",
    "isVirginica = False\n",
    "\n",
    "for specie in ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']:\n",
    "    if 'setosa' in specie:\n",
    "        classification_result_setosa = inst_setosa.classify(new_instance_3, tree3_1)\n",
    "        print('1->', classification_result_setosa)\n",
    "        isSetosa = True if classification_result_setosa == 'Iris-setosa' else False\n",
    "    if 'versicolor' in specie:\n",
    "        classification_result_versicolor = inst_versicolor.classify(new_instance_3, tree3_2)\n",
    "        print('2->', classification_result_versicolor)\n",
    "        isVersicolor = True if classification_result_versicolor == 'Iris-versicolor' else False\n",
    "    if 'virginica' in specie:\n",
    "        classification_result_virginica = inst_virginica.classify(new_instance_3, tree3_3)\n",
    "        print('3->', classification_result_virginica)\n",
    "        isVirginica = True if classification_result_virginica == 'Iris-virginica' else False\n",
    "\n",
    "num_trues = sum(x for x in [isSetosa, isVersicolor, isVirginica]) # sum of true values\n",
    "\n",
    "if num_trues == 1:\n",
    "    print(\"\\nValid classification.\")\n",
    "else:\n",
    "    print(\"\\nInvalid classification.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "source": [
    "\n",
    "#### Iris bonus test-split\n",
    "\n",
    "To double-check and evaluate the performance of those trees, is (since the dataset is bigger than previously) to **perform a train/test split**, to evaluate the accuracy of the decision tree created.\n",
    "\n",
    "The train_test split will be performed manually, without the use of *sklearn* module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3(ID3):\n",
    "    def _score(self, test_split_frac=0.5, random_seed=42):\n",
    "        \"\"\"Calculate and return the accuracy of the decision tree for Iris.\"\"\"\n",
    "\n",
    "        random.seed(random_seed)\n",
    "\n",
    "        df_shuffled = self.df.sample(random_state=random_seed).reset_index(drop=True)\n",
    "        indexes = self.df.index.tolist()\n",
    "        random.shuffle(indexes)\n",
    "        df_shuffled = self.df.loc[indexes].reset_index(drop=True)\n",
    "\n",
    "        # split the data, find the index using the fraction arg\n",
    "        split_index = int(len(df_shuffled) * (1 - test_split_frac))\n",
    "        train_data = df_shuffled[:split_index]\n",
    "        test_data = df_shuffled[split_index:]   \n",
    "\n",
    "        tree = self._build_tree(train_data) # tree build on the training data\n",
    "\n",
    "        # evaluate accuracy on the testing set, using same approach as 1.\n",
    "        correct_predictions = 0\n",
    "        for index, row in test_data.iterrows(): \n",
    "            predicted = self.classify(row, tree)\n",
    "            if predicted == row[self.target]:\n",
    "                correct_predictions += 1\n",
    "\n",
    "        accuracy = correct_predictions / len(test_data)\n",
    "        return accuracy\n",
    "\n",
    "tree_model = ID3(df3)  \n",
    "accuracy = tree_model._score(test_split_frac=0.3, random_seed=random.randint(100, 999)) # each time that it runs, it can give a different accuracy based on randomness\n",
    "print(f\"The accuracy of the model is: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect-4 Implementation\n",
    "\n",
    "This is the hardest part of the project. We need to create a class capable of holding the connect-4 game logic, and for that, we will be reusing the last project's code.\n",
    "\n",
    "Again, if we go and define a DT with no changes, a tree is generated, although its value is not relevant.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sampled = df4.sample(n=100, random_state=42) \n",
    "\n",
    "t4 = ID3(df_sampled)\n",
    "dt4 = t4._build_tree(df_sampled)\n",
    "t4.print_tree(dt4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the class Game, used on the other work. For that reason, we won't lose much time explaining the code, since that is NOT this project's objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self):\n",
    "        self.board = [['-' for _ in range(7)] for _ in range(6)]\n",
    "        self.turn = 'X'\n",
    "\n",
    "    def printer(self):\n",
    "        print(\" \".join([str(i) for i in range(7)]))\n",
    "        for line in self.board:\n",
    "            print(' '.join(line))\n",
    "\n",
    "    def swap(self):\n",
    "        if self.turn == 'X':\n",
    "            self.turn = 'O'\n",
    "        else:\n",
    "            self.turn = 'X'\n",
    "\n",
    "    def copy(self):\n",
    "        new_game = copy.deepcopy(self)\n",
    "        return new_game\n",
    "    \n",
    "    def full(self):\n",
    "        row1 = self.board[0] #only first row required, since it's impossible to have blank spaces in the middle of the board\n",
    "        if '-' in row1:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def check_line(self, line): \n",
    "        countX = 0\n",
    "        countO = 0\n",
    "        for i in range(len(line)):\n",
    "            if line[i] == 'X':\n",
    "                countX += 1\n",
    "                countO = 0\n",
    "            elif line[i] == 'O':\n",
    "                countO += 1\n",
    "                countX = 0\n",
    "            else:\n",
    "                countX = 0\n",
    "                countO = 0\n",
    "\n",
    "            if countX == 4:\n",
    "                return 'X'\n",
    "            elif countO == 4:\n",
    "                return 'O'\n",
    "        \n",
    "        return '-'\n",
    "    \n",
    "    def check_win(self):\n",
    "        # Check rows\n",
    "        for row in self.board:\n",
    "            result = self.check_line(row)\n",
    "            if result != '-':\n",
    "                return result\n",
    "\n",
    "        # Check columns\n",
    "        for i in range(7):\n",
    "            column = [self.board[j][i] for j in range(6)]\n",
    "            result = self.check_line(column)\n",
    "            if result != '-':\n",
    "                return result\n",
    "\n",
    "        # Check main diagonals /\n",
    "        main_diagonal_indices = [(2, 0), (1, 0), (0, 0), (0, 1), (0, 2), (0, 3)]\n",
    "        for start_row, start_col in main_diagonal_indices:\n",
    "            diagonal = []\n",
    "            i, j = start_row, start_col\n",
    "            while 0 <= i < 6 and 0 <= j < 7:\n",
    "                diagonal.append(self.board[i][j])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            result = self.check_line(diagonal)\n",
    "            if result != '-':\n",
    "                return result\n",
    "\n",
    "        # Check inverse diagonals \\\n",
    "        inverse_diagonal_indices = [(0, 3), (0, 4), (0, 5), (0, 6), (1, 6), (2, 6)]\n",
    "        for start_row, start_col in inverse_diagonal_indices:\n",
    "            diagonal = []\n",
    "            i, j = start_row, start_col\n",
    "            while 0 <= i < 6 and 0 <= j < 7:\n",
    "                diagonal.append(self.board[i][j])\n",
    "                i += 1\n",
    "                j -= 1\n",
    "            result = self.check_line(diagonal)\n",
    "            if result != '-':\n",
    "                return result\n",
    "\n",
    "        return 'U' # unclear winner until the moment\n",
    "    \n",
    "    def available_cols(self):\n",
    "        available_cols = []\n",
    "        for col in range(7):\n",
    "            if self.board[0][col] == '-':  \n",
    "                available_cols.append(col)\n",
    "        return available_cols\n",
    "\n",
    "    def isFinished(self):\n",
    "        if not self.full() and self.check_win()=='U':\n",
    "            return False \n",
    "        return True\n",
    "    \n",
    "    def play_pvp(self):\n",
    "        while not self.isFinished():\n",
    "            self.printer()\n",
    "            choice = int(input(f\"It is now {self.turn}'s turn.\\nMake a move by choosing your coordinates to play:\" ))\n",
    "            while not (0 <= choice < 7) or choice not in self.available_cols():                \n",
    "                choice = int(input(f\"Invalid move.\\nMake a move by choosing your coordinates to play:\" ))\n",
    "            self.make_move(choice)\n",
    "            self.swap()\n",
    "            # Check for a winner after each move\n",
    "            winner = self.check_win()\n",
    "            if winner == 'O':\n",
    "                self.printer()\n",
    "                print(\"O won!\")\n",
    "            elif winner == 'X':\n",
    "                self.printer()\n",
    "                print(\"X won!\")\n",
    "            elif self.isFinished() and winner == '-':\n",
    "                self.printer()\n",
    "                print(\"It's a tie!\")\n",
    "\n",
    "    def undo_move(self, c):\n",
    "        for i in range(6):\n",
    "            if self.board[i][c] != '-':\n",
    "                self.board[i][c] = '-'\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def make_move(self, y): \n",
    "        if self.board[0][y] != '-':\n",
    "            return -1\n",
    "        for i in range(5, -1, -1):\n",
    "            if self.board[i][y] == '-':\n",
    "                self.board[i][y] = self.turn\n",
    "                return 1\n",
    "        return 0       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the gameplay, we can play PvP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Game()\n",
    "#a.play_pvp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However there are a lot of changes required to make the \"AI\" work, to make, as the following:\n",
    "- Either the class is changed to sustain the new characters, or (our approach), replace explicitly on the dataframe, using \"replace()\" methods;\n",
    "\n",
    "- Make sure that the logical link between a line of the dataframe and an instance of the class is ensured;\n",
    "\n",
    "- The AI will consider our move in a Game instance. Then, it will consider the possible moves, using the *available_cols* method:\n",
    "  - it needs to get all the lines that have those moves placed;\n",
    "  - each line, each game is considered, to choose the next move, using some sort of minimax adapted to traverse the tree and weight the branches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) Replace the values on the dataframe to adapt to our class\n",
    "\n",
    "It will be called everytime a new instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_replace_df4():\n",
    "    \"\"\"Return df4 with the changes required to be in accordance with the class\"\"\"\n",
    "    df4 = pd.read_csv('connect4.csv', header=None) # reload for a fresh start\n",
    "    df4.replace({'o': 'O', 'x': 'X', 'b': '-'}, inplace=True)\n",
    "\n",
    "    # define the expected columns\n",
    "    expected_columns = [f\"{col}-{row}\" for col in range(7) for row in range(5, -1, -1)] + ['result']\n",
    "\n",
    "    # check if the first column is an ID column (non-string data)\n",
    "    if df4.iloc[:, 0].apply(lambda x: isinstance(x, int)).all():\n",
    "        df4.drop(columns=[0], inplace=True)\n",
    "        df4.columns = expected_columns\n",
    "    else:\n",
    "        df4.columns = expected_columns\n",
    "    \n",
    "    df4['result'] = df4['result'].replace('draw', 'loss')\n",
    "    \n",
    "    return df4\n",
    "\n",
    "df4 = create_replace_df4()\n",
    "df4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) Be able to instantiate a line of the dataframe into the class\n",
    "We deleted this part as it was conflicting with the 5). However later in the code, we might need to redefine the class to be able to do so. If the instance was defined with the board state being a part of the call, it work. The initialization would something like:\n",
    "\n",
    "```py\n",
    "'''\n",
    "def __init__(self, board = [['-' for _ in range(7)] for _ in range(6)], turn = 'X'):\n",
    "    self.board = board\n",
    "    self.turn = turn\n",
    "'''\n",
    "```\n",
    "Then the bottom part would work. Uncomment and copy to the init function for testing. This was useful to double-check the orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to create a matrix from a dataframe line\n",
    "def matrix_from_df_line(l):\n",
    "    \"\"\"Return matrix given a dataframe line (Connect-4 only)\"\"\"\n",
    "    row_values = df4.iloc[l]\n",
    "\n",
    "    board_rows = 6\n",
    "    board_cols = 7\n",
    "    connect_4_board = np.full((board_rows, board_cols), '-')  \n",
    "\n",
    "    # populate the matrix \n",
    "    for col in df4.columns[:-1]:  # exclude the last column 'result', \n",
    "        col_index, row_index = map(int, col.split('-'))   \n",
    "        connect_4_board[row_index, col_index] = row_values[col]\n",
    "\n",
    "    return connect_4_board\n",
    "\n",
    "# get the board state for the first row\n",
    "board_state = matrix_from_df_line(0) # np_array, not list\n",
    "print(board_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) Find whose move is it, given the instant\n",
    "Can be useful, and works with the stated in 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "class Game(Game):\n",
    "    def set_turn(self, board): \n",
    "        flat_board = [cell for row in board for cell in row]\n",
    "        counts = Counter(flat_board)\n",
    "        if counts['O'] < counts['X']:\n",
    "            self.turn = 'O'\n",
    "        else:\n",
    "            self.turn = 'X'\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Be able to find all the lines that have a X (my turn) on the respective place\n",
    "\n",
    "Essentially, let's say I play on a new board state, in the column 3. We need all the lines in which I played there. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = create_replace_df4()\n",
    "\n",
    "filtered_df = df4[df4['3-5'] == 'X']\n",
    "filtered_df['3-5']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) Automatize the process\n",
    "What this does is that so when I play, the program records the move in a external dictionary, and then filters the dataframe based on that. \n",
    "\n",
    "We will overwrite the function make_move, to return the row in which the piece stayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = 3 # will correspond to the user move input/pc\n",
    "\n",
    "class Game(Game):\n",
    "    def make_move(self, y): \n",
    "        if self.board[0][y] != '-':\n",
    "            return -1\n",
    "        for i in range(5, -1, -1):\n",
    "            if self.board[i][y] == '-':\n",
    "                self.board[i][y] = self.turn\n",
    "                return i\n",
    "        return -1\n",
    "\n",
    "def test():\n",
    "    t = Game()\n",
    "    #t.printer()\n",
    "    x = t.make_move(3)\n",
    "    print(x)\n",
    "    #t.printer()\n",
    "\n",
    "test()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can access the move coordinates, it's easy to add it to the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "played_moves = {}\n",
    "\n",
    "def test2():\n",
    "    t = Game()\n",
    "    \n",
    "    col = int(input('Test column choice 0-6')) # no exception inputs since this is temporary\n",
    "    row = t.make_move(col)\n",
    "    form_str = f\"{col}-{row}\"\n",
    "\n",
    "    t.printer()\n",
    "    played_moves[form_str] = t.turn\n",
    "    t.swap()\n",
    "\n",
    "#test2() # uncomment\n",
    "print(played_moves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6) Make the *play* function\n",
    "The function will receive continuous inputs, in this case for both. The input choice for 'O' will later be replaced for the IG for loss/draw, representing the PC move.\n",
    "\n",
    "This ended up never being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Game()\n",
    "\n",
    "played_moves = {}\n",
    "\n",
    "class Game(Game):\n",
    "    def play_temp(self):\n",
    "        while not self.isFinished(): # still missing who won, not important here\n",
    "            my_move_col = int(input())\n",
    "            my_move_row = t.make_move(my_move_col)\n",
    "            form_str = f\"{my_move_col}-{my_move_row}\"\n",
    "            played_moves[form_str] = t.turn\n",
    "\n",
    "            t.printer()\n",
    "            print(played_moves)\n",
    "            print()\n",
    "            t.swap()\n",
    "\n",
    "            pc_move_col = int(input())\n",
    "            pc_move_row = t.make_move(pc_move_col)\n",
    "            form_str = f\"{pc_move_col}-{pc_move_row}\"\n",
    "            played_moves[form_str] = t.turn\n",
    "\n",
    "            t.printer()\n",
    "            print(played_moves)\n",
    "            print()\n",
    "            t.swap()\n",
    "\n",
    "#t.play_temp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) Intersect the dictionary with the dataset\n",
    "\n",
    "In each iteration, we need to intersect the dictionary. We will perform deepcopies of the  data subset continuosly diminishing.\n",
    "\n",
    "Essentially, what we need is to join the previous methodologies, described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "played_moves = {'0-5': 'X', '1-5': 'O', '2-5': 'X'} # example\n",
    "\n",
    "for key in played_moves:\n",
    "    df4 = df4[df4[key]==played_moves[key]]\n",
    "    print(len(df4))\n",
    "\n",
    "#df4.head(10) # checking if it works, uncomment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8) Find next avaliable moves, given a *Game* instance\n",
    "\n",
    "Parse them to a dictionary that links coordinate to the turn there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Game()\n",
    "t.make_move(3)\n",
    "t.printer()\n",
    "t.swap()\n",
    "\n",
    "\n",
    "class Game(Game):\n",
    "    def getdict_available_moves(self):\n",
    "        possible_moves = {}\n",
    "        avail_cols = t.available_cols()\n",
    "        for col in avail_cols:\n",
    "            row = self.make_move(col)\n",
    "            self.undo_move(col)\n",
    "            form_str = f\"{col}-{row}\"\n",
    "            possible_moves[form_str] = self.turn\n",
    "\n",
    "        return possible_moves\n",
    "\n",
    "possible_moves = t.getdict_available_moves()\n",
    "possible_moves\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9) We need IG for each dictionary\n",
    "Meaning we will need to test (max) 7 times to check for a dictionary containing the previous moves, plus one of the available, and see what move is the most favourable for splitting.\n",
    "\n",
    "To make the target binary, and since the PC is always the second, we consider a draw to be more favourable to the PC, the same way as in *tic-tac-toe*.\n",
    "\n",
    "That addition was held in the *create_replace()* method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to restart all the changes that already took place. We will also create a new instance, and try to get all the possible moves, associated to the IG of that move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = create_replace_df4()\n",
    "\n",
    "t = Game()\n",
    "t.make_move(3)\n",
    "t.swap()\n",
    "\n",
    "\n",
    "previous_moves = {'3-5':'X'}\n",
    "possible_moves = t.getdict_available_moves()\n",
    "possible_moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, possible_moves is a dictionary that stores all the possible moves.\n",
    "We need to fill the IG, based on the list_keys availability, target (loss), etc.\n",
    "\n",
    "It's therefore required to make an adapted version of the *_choose_best_attribute* method.\n",
    "\n",
    "Instead of overwriting again, we will define specific functions for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3(ID3):            \n",
    "    def _find_best_column_to_split(self, possible_moves, previous_moves):\n",
    "        \"\"\"Returns, given the current game state, and the past moves, the (col-row) that is presented as the most favourable (highest IG).\"\"\"\n",
    "        dict_move_ig = {}\n",
    "        \n",
    "        for move_key in possible_moves:  # for each possible move in that state\n",
    "            updated_moves = copy.deepcopy(previous_moves)\n",
    "            updated_moves[move_key] = possible_moves[move_key]  # copy of previous moves, plus the move we are testing\n",
    "            \n",
    "            # t will be the subset\n",
    "            t = copy.deepcopy(self.df)\n",
    "            \n",
    "            # filter t\n",
    "            for key, value in updated_moves.items(): # for pair in the items of the dict\n",
    "                t = t[t[key] == value] # same as point 4)\n",
    "                \n",
    "            ig = self._information_gain(t, self.target)\n",
    "            dict_move_ig[move_key] = ig \n",
    "        \n",
    "        max_ig_move = max(dict_move_ig, key=dict_move_ig.get) # same code as in restaurant\n",
    "        print(dict_move_ig)\n",
    "        return max_ig_move\n",
    "\n",
    "i = ID3(df4)\n",
    "#i.generate_tree_connect4()\n",
    "\n",
    "max_ig = i._find_best_column_to_split(possible_moves, previous_moves)\n",
    "max_ig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10) Creating a tree\n",
    "Given that, the IG now makes sense, meaning now it can't/won't be aplied to floating pieces. We can probably try to build a tree based on the functions stated above. However we couldn't.\n",
    "\n",
    " We will be defining a **new *_build_tree*** function instead of overwriting, although the changes could be called as arguments, but it's way more complicated.\n",
    "\n",
    "- The new tree will be given as **attribute** the **past moves**, so that recursively it updates the Game instance, and also intersects the dataframe;\n",
    "- The **ending/base cases need to change**;\n",
    "- The values on the nested dictionary might need to be updates as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game: # has the new updates stated in 5) and 8)\n",
    "    def __init__(self, board = [['-' for _ in range(7)] for _ in range(6)]):\n",
    "        self.board = board\n",
    "        self.turn = 'X' # by default\n",
    "\n",
    "    def printer(self):\n",
    "        print(\" \".join([str(i) for i in range(7)]))\n",
    "        for line in self.board:\n",
    "            print(' '.join(line))\n",
    "\n",
    "    def swap(self):\n",
    "        if self.turn == 'X':\n",
    "            self.turn = 'O'\n",
    "        else:\n",
    "            self.turn = 'X'\n",
    "\n",
    "    def copy(self):\n",
    "        new_game = copy.deepcopy(self)\n",
    "        return new_game\n",
    "    \n",
    "    def full(self):\n",
    "        row1 = self.board[0] #only first row required, since it's impossible to have blank spaces in the middle of the board\n",
    "        if '-' in row1:\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def check_line(self, line): \n",
    "        countX = 0\n",
    "        countO = 0\n",
    "        for i in range(len(line)):\n",
    "            if line[i] == 'X':\n",
    "                countX += 1\n",
    "                countO = 0\n",
    "            elif line[i] == 'O':\n",
    "                countO += 1\n",
    "                countX = 0\n",
    "            else:\n",
    "                countX = 0\n",
    "                countO = 0\n",
    "\n",
    "            if countX == 4:\n",
    "                return 'X'\n",
    "            elif countO == 4:\n",
    "                return 'O'\n",
    "        \n",
    "        return '-'\n",
    "    \n",
    "    def check_win(self):\n",
    "        # Check rows\n",
    "        for row in self.board:\n",
    "            result = self.check_line(row)\n",
    "            if result != '-':\n",
    "                return result\n",
    "\n",
    "        # Check columns\n",
    "        for i in range(7):\n",
    "            column = [self.board[j][i] for j in range(6)]\n",
    "            result = self.check_line(column)\n",
    "            if result != '-':\n",
    "                return result\n",
    "\n",
    "        # Check main diagonals /\n",
    "        main_diagonal_indices = [(2, 0), (1, 0), (0, 0), (0, 1), (0, 2), (0, 3)]\n",
    "        for start_row, start_col in main_diagonal_indices:\n",
    "            diagonal = []\n",
    "            i, j = start_row, start_col\n",
    "            while 0 <= i < 6 and 0 <= j < 7:\n",
    "                diagonal.append(self.board[i][j])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            result = self.check_line(diagonal)\n",
    "            if result != '-':\n",
    "                return result\n",
    "\n",
    "        # Check inverse diagonals \\\n",
    "        inverse_diagonal_indices = [(0, 3), (0, 4), (0, 5), (0, 6), (1, 6), (2, 6)]\n",
    "        for start_row, start_col in inverse_diagonal_indices:\n",
    "            diagonal = []\n",
    "            i, j = start_row, start_col\n",
    "            while 0 <= i < 6 and 0 <= j < 7:\n",
    "                diagonal.append(self.board[i][j])\n",
    "                i += 1\n",
    "                j -= 1\n",
    "            result = self.check_line(diagonal)\n",
    "            if result != '-':\n",
    "                return result\n",
    "\n",
    "        return 'U' # unclear winner until the moment\n",
    "    \n",
    "    def available_cols(self):\n",
    "        available_cols = []\n",
    "        for col in range(7):\n",
    "            if self.board[0][col] == '-':  \n",
    "                available_cols.append(col)\n",
    "        return available_cols\n",
    "\n",
    "    def isFinished(self):\n",
    "        if not self.full() and self.check_win()=='U':\n",
    "            return False \n",
    "        return True\n",
    "    \n",
    "    def play_pvp(self):\n",
    "        while not self.isFinished():\n",
    "            self.printer()\n",
    "            choice = int(input(f\"It is now {self.turn}'s turn.\\nMake a move by choosing your coordinates to play:\" ))\n",
    "            while not (0 <= choice < 7) or choice not in self.available_cols():                \n",
    "                choice = int(input(f\"Invalid move.\\nMake a move by choosing your coordinates to play:\" ))\n",
    "            self.make_move(choice)\n",
    "            self.swap()\n",
    "            # Check for a winner after each move\n",
    "            winner = self.check_win()\n",
    "            if winner == 'O':\n",
    "                self.printer()\n",
    "                print(\"O won!\")\n",
    "            elif winner == 'X':\n",
    "                self.printer()\n",
    "                print(\"X won!\")\n",
    "            elif self.isFinished() and winner == '-':\n",
    "                self.printer()\n",
    "                print(\"It's a tie!\")\n",
    "\n",
    "    def undo_move(self, c):\n",
    "        for i in range(6):\n",
    "            if self.board[i][c] != '-':\n",
    "                self.board[i][c] = '-'\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def make_move(self, y): \n",
    "        if self.board[0][y] != '-':\n",
    "            return -1\n",
    "        for i in range(5, -1, -1):\n",
    "            if self.board[i][y] == '-':\n",
    "                self.board[i][y] = self.turn\n",
    "                return i\n",
    "        return -1      \n",
    "    \n",
    "    def getdict_available_moves(self):\n",
    "        possible_moves = {}\n",
    "        avail_cols = t.available_cols()\n",
    "        for col in avail_cols:\n",
    "            row = self.make_move(col)\n",
    "            self.undo_move(col)\n",
    "            form_str = f\"{col}-{row}\"\n",
    "            possible_moves[form_str] = self.turn\n",
    "\n",
    "        return possible_moves\n",
    "\n",
    "    def set_turn(self, board): \n",
    "        flat_board = [cell for row in board for cell in row]\n",
    "        counts = Counter(flat_board)\n",
    "        if counts['O'] < counts['X']:\n",
    "            self.turn = 'O'\n",
    "        else:\n",
    "            self.turn = 'X'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One very important step that's required, is not only intersect a dictionary with the dataframe, but also intersect a dictionary of the past moves with the class, to check for available columns.\n",
    "\n",
    "*We added set_turn to the class above to help.*\n",
    "\n",
    "Let's check that really quick before we proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = create_replace_df4()\n",
    "\n",
    "def is_valid_key(str_x):\n",
    "    n = 0\n",
    "    for elem in str_x:\n",
    "        if elem == '-':\n",
    "            n += 1\n",
    "    \n",
    "    if n==1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\"\"\"\n",
    "# instance from line\n",
    "np_matrix = matrix_from_df_line(0)\n",
    "board_test = np_matrix.tolist()In kNN, overfitting\n",
    "\n",
    "inst = Game(board_test)\n",
    "inst.printer()\n",
    "inst.set_turn(board_test)\n",
    "print(inst.turn)\n",
    "\"\"\"\n",
    "# instance from past moves dict\n",
    "\n",
    "def create_instance_from_dict(prev):\n",
    "    \"\"\"Given a dictionary with the played moves, returns a Game instance following that board.\"\"\"\n",
    "    new_board = [['-' for _ in range(7)] for _ in range(6)]\n",
    "    for key in prev:\n",
    "        if is_valid_key(key):\n",
    "            a, b = key.split('-')\n",
    "            new_board[int(b)][int(a)] = prev[key]\n",
    "    t = Game(new_board)\n",
    "    t.set_turn(t.board)\n",
    "    return t\n",
    "\n",
    "# Assuming you have the rest of the Game class defined elsewhere\n",
    "prev_moves = {'3-5':'X', '3-4':'O', '2-5':'X'}\n",
    "inst2 = create_instance_from_dict(prev_moves)\n",
    "inst2.printer()\n",
    "print(inst2.turn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can create a game instance from past moves, that logic can be used in the build tree, to recursively update the legal moves, and build the tree based on that.\n",
    "\n",
    "Here is the **pseudocode** to the idea of generating a tree that only **opens nodes for legal moves**.\n",
    "\n",
    "```txt\n",
    "function buildtree(past_moves):\n",
    "    subset = intersect(past_moves, copy(dataframe))\n",
    "    \n",
    "    if base_case(past_moves/subset):\n",
    "        return value of node\n",
    "    \n",
    "    inst = create_instance(past_moves)\n",
    "\n",
    "    # find IG for each move\n",
    "    max_ig = i._find_best_column_to_split(possible_moves, previous_moves)\n",
    "    split\n",
    "    \n",
    "```\n",
    "\n",
    "The code will NOT be optimized, since we will use a lot of deepcopies and intersect continuously, that however makes it harder to debug and fix.\n",
    "\n",
    "**If needed, the changes required to make it faster and less memory-expensive are to parse the dataset subset as an argument, and the past moves becomes a single element/pair dictionary containing the last move, since the other past moves were already intersected with the dataframe subset.** \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3(ID3):\n",
    "    def build_tree_c4(self):\n",
    "        past_moves = {}\n",
    "        \"\"\"\n",
    "        Constructs the nested dictionary for connect-4.\n",
    "        The argument past_moves is a dictionary 'coordinate':'turn' used for recursion.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create subset, intersect\n",
    "        copy_df = deepcopy(self.df)\n",
    "        if past_moves:  # not empty, first iteration          \n",
    "            for key in past_moves: # valid key, 0--1 happening for unknown reasons\n",
    "                if is_valid_key(key):\n",
    "                    copy_df = copy_df[copy_df[key] == past_moves[key]]\n",
    "        \n",
    "        # Create game instance\n",
    "        game_instance = create_instance_from_dict(past_moves)\n",
    "        game_instance.set_turn(game_instance.board)\n",
    "\n",
    "        # Base cases (can't be the same)\n",
    "        # ?\n",
    "\n",
    "\n",
    "        # Iteration and recursion\n",
    "        next_poss_moves = game_instance.getdict_available_moves()\n",
    "        best_attribute = self._find_best_column_to_split(next_poss_moves, prev_moves)\n",
    "\n",
    "        node = {best_attribute: {}}\n",
    "\n",
    "        for value in ['X', 'O']:\n",
    "            subset = copy_df[copy_df[best_attribute] == value]\n",
    "\n",
    "            if subset.empty:\n",
    "                majority_class = class_counts.most_common(1)[0][0]\n",
    "                node[best_attribute][value] = [majority_class, 0]\n",
    "            else:\n",
    "                node[best_attribute][value] = self._build_tree_c4(prev_moves.update)\n",
    "        \n",
    "        return node\n",
    "    \n",
    "df4 = create_replace_df4()\n",
    "i = ID3(df4)\n",
    "#tree = i.build_tree_c4()\n",
    "#print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Since we couldn't define the base cases, mostly due to the lack of time, the recursion didn't work and therefore the tree didn't work. However all the needed functions for the matter are properly constructed within the steps 1-10.\n",
    "\n",
    "The objective of the last part was to **be able to create a tree that only opens available columns**. Meaning instead of the \"randomly\" generated tree in the beggining of the connect-4 implementation, we would have a tree that made sense.\n",
    "\n",
    "#### 11) After the tree\n",
    "\n",
    "In the end, using a **minimax adapted implementation**, we could utilize the **tree to choose the best move**, following the nested tree path and making the decision based on the outcome of those same paths.\n",
    "\n",
    "After the base case being held, meaning after the tree expansion ended (since the csv does NOT give ended games), we could use some sort of A* for the later moves, or MCTS, given that the computer has now some advantage in theory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test new Datasets<a name=\"inputs\"></a>\n",
    "[[go back to the top]](#contents)\n",
    "\n",
    "The code above can be used and aplied to new datasets in order to print the script below. This assumes no None values. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_multiclass(df):\n",
    "    \"\"\"Returns true if the target column of the arg dataset has true.\"\"\"\n",
    "    unique_values = df.iloc[:, -1].unique().tolist()\n",
    "\n",
    "    if len(unique_values) == 2:\n",
    "        \n",
    "        return False, [] # binary\n",
    "    else:\n",
    "        \n",
    "        return True, unique_values\n",
    "\n",
    "def tree_printer(df):\n",
    "    \"\"\"Print the tree, or various if multiclass.\"\"\"\n",
    "    print(\"The generated tree(s) are: \\n\")\n",
    "    bool_val, l = check_multiclass(df)\n",
    "    if not bool_val:\n",
    "        #print(\"binary\\n\")\n",
    "        df5_inst = ID3(df)\n",
    "        tree = df5_inst._build_tree(df5_inst.df)\n",
    "        df5_inst.print_tree(tree)\n",
    "    else:\n",
    "        unique_values = l\n",
    "        #print(\"non binary\\n\")\n",
    "\n",
    "        dictX = {}\n",
    "        for x in unique_values:\n",
    "            dictX[x] = False\n",
    "            df5_copy = deepcopy(df5)\n",
    "\n",
    "            last_column = df5_copy.iloc[:, -1]\n",
    "            df5_copy.iloc[:, -1] = last_column.where(last_column == x, 'not ' + str(x))\n",
    "\n",
    "            df5_inst = ID3(df5_copy)\n",
    "            tree = df5_inst._build_tree(df5_inst.df)\n",
    "            print(x + \":\\n\")\n",
    "            df5_inst.print_tree(tree)\n",
    "            print(\"\\n-------------------------------------\\n\")\n",
    "        \n",
    "    return tree, bool_val # true if multiclass\n",
    "\n",
    "def tree_no_printer(df):\n",
    "    print(\"The generated tree(s) are: \\n\")\n",
    "    bool_val, l = check_multiclass(df)\n",
    "    if not bool_val:\n",
    "        #print(\"binary\\n\")\n",
    "        df5_inst = ID3(df)\n",
    "        tree = df5_inst._build_tree(df5_inst.df)\n",
    "        #df5_inst.print_tree(tree)\n",
    "    else:\n",
    "        unique_values = l\n",
    "        #print(\"non binary\\n\")\n",
    "\n",
    "        dictX = {}\n",
    "        for x in unique_values:\n",
    "            dictX[x] = False\n",
    "            df5_copy = deepcopy(df5)\n",
    "\n",
    "            last_column = df5_copy.iloc[:, -1]\n",
    "            df5_copy.iloc[:, -1] = last_column.where(last_column == x, 'not ' + str(x))\n",
    "\n",
    "            df5_inst = ID3(df5_copy)\n",
    "            tree = df5_inst._build_tree(df5_inst.df)\n",
    "            #print(x + \":\\n\")\n",
    "            #df5_inst.print_tree(tree)\n",
    "            #print(\"\\n-------------------------------------\\n\")\n",
    "        \n",
    "    return tree, bool_val # true if multiclass\n",
    "\n",
    "#tree, t = tree_printer(df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to classify a new csv, **with the same header**, the name of the csv can be parsed, and then the next script will classify every instance in that same file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14237/3217669587.py:54: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cols_to_replace.replace(to_replace, replacement, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The generated tree(s) are: \n",
      "\n",
      "{\n",
      "      'Pat': {\n",
      "            'Some': 'Yes',\n",
      "            'Full': {\n",
      "                  'Hun': {\n",
      "                        'Yes': {\n",
      "                              'Type': {\n",
      "                                    'French': 'No',\n",
      "                                    'Thai': {\n",
      "                                          'Fri': {\n",
      "                                                'No': 'No',\n",
      "                                                'Yes': 'Yes'\n",
      "                                          }\n",
      "                                    },\n",
      "                                    'Burger': 'Yes',\n",
      "                                    'Italian': 'No'\n",
      "                              }\n",
      "                        },\n",
      "                        'No': 'No'\n",
      "                  }\n",
      "            },\n",
      "            'None': 'No'\n",
      "      }\n",
      "}\n",
      "['X1', 'Yes', 'No', 'No', 'Yes', 'Some', '$$$', 'No', 'Yes', 'French', '0-10', 'Yes']\n",
      "{'ID': 'X1', 'Alt': 'Yes', 'Bar': 'No', 'Fri': 'No', 'Hun': 'Yes', 'Pat': 'Some', 'Price': '$$$', 'Rain': 'No', 'Res': 'Yes', 'Type': 'French', 'Est': '0-10', 'Class': 'Yes'}\n",
      "Classification result: Yes\n",
      "\n",
      "['X2', 'Yes', 'No', 'No', 'Yes', 'Full', '$', 'No', 'No', 'Thai', '30-60', 'No']\n",
      "{'ID': 'X2', 'Alt': 'Yes', 'Bar': 'No', 'Fri': 'No', 'Hun': 'Yes', 'Pat': 'Full', 'Price': '$', 'Rain': 'No', 'Res': 'No', 'Type': 'Thai', 'Est': '30-60', 'Class': 'No'}\n",
      "Classification result: No\n",
      "\n",
      "['X3', 'No', 'Yes', 'No', 'No', 'Some', '$', 'No', 'No', 'Burger', '0-10', 'Yes']\n",
      "{'ID': 'X3', 'Alt': 'No', 'Bar': 'Yes', 'Fri': 'No', 'Hun': 'No', 'Pat': 'Some', 'Price': '$', 'Rain': 'No', 'Res': 'No', 'Type': 'Burger', 'Est': '0-10', 'Class': 'Yes'}\n",
      "Classification result: Yes\n",
      "\n",
      "['X4', 'Yes', 'No', 'Yes', 'Yes', 'Full', '$', 'No', 'No', 'Thai', '10-30', 'Yes']\n",
      "{'ID': 'X4', 'Alt': 'Yes', 'Bar': 'No', 'Fri': 'Yes', 'Hun': 'Yes', 'Pat': 'Full', 'Price': '$', 'Rain': 'No', 'Res': 'No', 'Type': 'Thai', 'Est': '10-30', 'Class': 'Yes'}\n",
      "Classification result: Yes\n",
      "\n",
      "['X5', 'Yes', 'No', 'Yes', 'No', 'Full', '$$$', 'No', 'Yes', 'French', '>60', 'No']\n",
      "{'ID': 'X5', 'Alt': 'Yes', 'Bar': 'No', 'Fri': 'Yes', 'Hun': 'No', 'Pat': 'Full', 'Price': '$$$', 'Rain': 'No', 'Res': 'Yes', 'Type': 'French', 'Est': '>60', 'Class': 'No'}\n",
      "Classification result: No\n",
      "\n",
      "['X6', 'No', 'Yes', 'No', 'Yes', 'Some', '$$', 'Yes', 'Yes', 'Italian', '0-10', 'Yes']\n",
      "{'ID': 'X6', 'Alt': 'No', 'Bar': 'Yes', 'Fri': 'No', 'Hun': 'Yes', 'Pat': 'Some', 'Price': '$$', 'Rain': 'Yes', 'Res': 'Yes', 'Type': 'Italian', 'Est': '0-10', 'Class': 'Yes'}\n",
      "Classification result: Yes\n",
      "\n",
      "['X7', 'No', 'Yes', 'No', 'No', 'None', '$', 'Yes', 'No', 'Burger', '0-10', 'No']\n",
      "{'ID': 'X7', 'Alt': 'No', 'Bar': 'Yes', 'Fri': 'No', 'Hun': 'No', 'Pat': 'None', 'Price': '$', 'Rain': 'Yes', 'Res': 'No', 'Type': 'Burger', 'Est': '0-10', 'Class': 'No'}\n",
      "Classification result: No\n",
      "\n",
      "['X8', 'No', 'No', 'No', 'Yes', 'Some', '$$', 'Yes', 'Yes', 'Thai', '0-10', 'Yes']\n",
      "{'ID': 'X8', 'Alt': 'No', 'Bar': 'No', 'Fri': 'No', 'Hun': 'Yes', 'Pat': 'Some', 'Price': '$$', 'Rain': 'Yes', 'Res': 'Yes', 'Type': 'Thai', 'Est': '0-10', 'Class': 'Yes'}\n",
      "Classification result: Yes\n",
      "\n",
      "['X9', 'No', 'Yes', 'Yes', 'No', 'Full', '$', 'Yes', 'No', 'Burger', '>60', 'No']\n",
      "{'ID': 'X9', 'Alt': 'No', 'Bar': 'Yes', 'Fri': 'Yes', 'Hun': 'No', 'Pat': 'Full', 'Price': '$', 'Rain': 'Yes', 'Res': 'No', 'Type': 'Burger', 'Est': '>60', 'Class': 'No'}\n",
      "Classification result: No\n",
      "\n",
      "['X10', 'Yes', 'Yes', 'Yes', 'Yes', 'Full', '$$$', 'No', 'Yes', 'Italian', '10-30', 'No']\n",
      "{'ID': 'X10', 'Alt': 'Yes', 'Bar': 'Yes', 'Fri': 'Yes', 'Hun': 'Yes', 'Pat': 'Full', 'Price': '$$$', 'Rain': 'No', 'Res': 'Yes', 'Type': 'Italian', 'Est': '10-30', 'Class': 'No'}\n",
      "Classification result: No\n",
      "\n",
      "['X11', 'No', 'No', 'No', 'No', 'None', '$', 'No', 'No', 'Thai', '0-10', 'No']\n",
      "{'ID': 'X11', 'Alt': 'No', 'Bar': 'No', 'Fri': 'No', 'Hun': 'No', 'Pat': 'None', 'Price': '$', 'Rain': 'No', 'Res': 'No', 'Type': 'Thai', 'Est': '0-10', 'Class': 'No'}\n",
      "Classification result: No\n",
      "\n",
      "['X12', 'Yes', 'Yes', 'Yes', 'Yes', 'Full', '$', 'No', 'No', 'Burger', '30-60', 'Yes']\n",
      "{'ID': 'X12', 'Alt': 'Yes', 'Bar': 'Yes', 'Fri': 'Yes', 'Hun': 'Yes', 'Pat': 'Full', 'Price': '$', 'Rain': 'No', 'Res': 'No', 'Type': 'Burger', 'Est': '30-60', 'Class': 'Yes'}\n",
      "Classification result: Yes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def classify_bin(df_instance, tree, bool_value, filename): #\n",
    "    input_file = filename\n",
    "    \n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        content = file.readlines()\n",
    "\n",
    "    first_line = content[0].strip().split(',')  \n",
    "\n",
    "    for line in content[1:]:\n",
    "        line_parser = line.strip().split(',')\n",
    "        \n",
    "        # Check if the lengths match before proceeding\n",
    "        if len(first_line) == len(line_parser):\n",
    "            d = {}\n",
    "            for i in range(len(line_parser)):\n",
    "                d[first_line[i]] = line_parser[i]\n",
    "            \n",
    "            \n",
    "            print(line_parser)\n",
    "            print(d)\n",
    "            if not bool_value:\n",
    "                classification = df_instance._classify(d, tree)\n",
    "                print(f\"Classification result: {classification}\\n\")\n",
    "        else:\n",
    "            print(\"Length mismatch between header and data in the input file.\")\n",
    "\n",
    "def replace_boolean_values(csv_file):\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Define the values to replace and their replacement\n",
    "    to_replace = [\"FALSE\", \"False\"]\n",
    "    replacement = \"False\"\n",
    "\n",
    "    # Replace values in the DataFrame\n",
    "    df.replace(to_replace, replacement, inplace=True)\n",
    "\n",
    "    # Write the modified DataFrame back to the CSV file\n",
    "    df.to_csv(\"temp.txt\", index=False)\n",
    "\n",
    "def full_test():\n",
    "    name_of_csv = str(input(\"Write the name of the csv, the file should be stored in the same directory as this notebook. \"))\n",
    "    df5 = pd.read_csv(name_of_csv)\n",
    "    \n",
    "    #########\n",
    "    # PREPROCESSING\n",
    "    df5 = df5.drop(\"ID\", axis=1) # comment if it makes sense\n",
    "    to_replace = [\"False\", \"FALSE\"]\n",
    "    replacement = \"False\"\n",
    "    \n",
    "    cols_to_replace = df5.iloc[:, :-1]\n",
    "\n",
    "    # Replace values in the selected columns\n",
    "    cols_to_replace.replace(to_replace, replacement, inplace=True)\n",
    "\n",
    "    # Concatenate the modified columns with the original last column\n",
    "    df5 = pd.concat([cols_to_replace, df5.iloc[:, -1]], axis=1)\n",
    "    #########\n",
    "    \n",
    "    df5_inst = ID3(df5)\n",
    "\n",
    "    bool_print_tree = str(input(\"Want to also print tree? (y/n)\"))\n",
    "    if bool_print_tree == \"y\":\n",
    "        tree, b = tree_printer(df5)\n",
    "    elif bool_print_tree == \"n\":\n",
    "        tree, b = tree_no_printer(df5)\n",
    "        print(\"NOT REQUESTED\")\n",
    "    \n",
    "    #b = true if multiclass\n",
    "\n",
    "    test_csv = str(input(\"\\nInput/test file name: \"))\n",
    "\n",
    "    if not b:\n",
    "        replace_boolean_values(test_csv)\n",
    "        classify_bin(df5_inst, tree, False, \"temp.txt\")\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "full_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-> Iris-setosa\n",
      "2-> Not-Iris-versicolor\n",
      "3-> Not-Iris-virginica\n",
      "\n",
      "Valid classification.\n"
     ]
    }
   ],
   "source": [
    "isSetosa = False\n",
    "isVersicolor = False\n",
    "isVirginica = False\n",
    "\n",
    "for specie in ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']:\n",
    "    if 'setosa' in specie:\n",
    "        classification_result_setosa = inst_setosa.classify(new_instance_3, tree3_1)\n",
    "        print('1->', classification_result_setosa)\n",
    "        isSetosa = True if classification_result_setosa == 'Iris-setosa' else False\n",
    "    if 'versicolor' in specie:\n",
    "        classification_result_versicolor = inst_versicolor.classify(new_instance_3, tree3_2)\n",
    "        print('2->', classification_result_versicolor)\n",
    "        isVersicolor = True if classification_result_versicolor == 'Iris-versicolor' else False\n",
    "    if 'virginica' in specie:\n",
    "        classification_result_virginica = inst_virginica.classify(new_instance_3, tree3_3)\n",
    "        print('3->', classification_result_virginica)\n",
    "        isVirginica = True if classification_result_virginica == 'Iris-virginica' else False\n",
    "\n",
    "num_trues = sum(x for x in [isSetosa, isVersicolor, isVirginica]) # sum of true values\n",
    "\n",
    "if num_trues == 1:\n",
    "    print(\"\\nValid classification.\")\n",
    "else:\n",
    "    print(\"\\nInvalid classification.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion <a name=\"conclusions\"></a>\n",
    "[[go back to the top]](#contents)\n",
    "\n",
    "In conclusion, the implementation and analysis of the ID3 algorithm for tre\n",
    "e creation have **showcased its effectiveness in generating decision trees** from datasets.\n",
    "\n",
    "Through its recursive partitioning based on attribute selection, the ID3 algorithm has demonstrated its capability to efficiently classify data and infer decision rules.\n",
    "\n",
    "Furthermore, one the most important things is that the flexibility of the ID3 algorithm allows for its adaptation to various domains and datasets, with very different characteristics among themselves.\n",
    "\n",
    "This work was really challenging, mainly the 4th dataset that we could not conclude. However we believe that it was really helpful in further learning about AI and its applications, since it's the first time we implemented something alike."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
